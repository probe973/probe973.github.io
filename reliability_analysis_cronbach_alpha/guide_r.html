<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introductory Statistics Guide: Reliability Analysis & Cronbach's Alpha in R</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            margin: 0;
            padding: 0;
            color: #222;
            background-color: #f8f8f8;
            line-height: 1.6;
        }
        .container {
            padding: 25px;
            max-width: 960px;
            margin: 30px auto;
            background-color: white;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            border-radius: 10px;
        }
        h1 {
            color: #004085;
            border-bottom: 3px solid #004085;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 2.2em;
        }
        h2 {
            color: #004085;
            margin-top: 35px;
            font-size: 1.8em;
            border-bottom: 2px solid #eee;
            padding-bottom: 5px;
        }
        h3 {
            color: #0056b3;
            margin-top: 25px; /* Adjusted to match other guides' h3 */
            font-size: 1.2em;
            border-bottom: 1px solid #eee;
            padding-bottom: 5px; /* Added consistent padding */
            margin-bottom: 10px;
        }
        p {
            margin-bottom: 20px;
        }
        code {
            background-color: #eef;
            padding: 2px 4px;
            border-radius: 4px;
            font-family: "Courier New", Courier, monospace;
            color: #c00;
        }
        pre {
            background-color: #eef;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
            font-family: "Courier New", Courier, monospace;
            color: #333;
            border: 1px solid #ddd;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        table, th, td {
            border: 1px solid #ddd;
        }
        th, td {
            padding: 10px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
            color: #333;
            font-weight: bold;
        }
        /* Top navigation style */
        .top-nav {
            display: flex;
            justify-content: flex-start;
            gap: 15px;
            padding: 15px;
            background-color: #004085;
            border-bottom-left-radius: 10px;
            border-bottom-right-radius: 10px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            align-items: center;
            position: sticky; /* Make it sticky */
            top: 0; /* Stick to the top */
            width: 100%; /* Ensure full width when sticky */
            z-index: 1000; /* Ensure it stays on top of other content */
        }
        .top-nav span {
            color: white;
            font-weight: bold;
            margin-right: 5px;
        }
        .top-nav a {
            color: white;
            padding: 8px 15px;
            border-radius: 20px;
            text-decoration: none;
            font-weight: bold;
            transition: background-color 0.3s ease, transform 0.2s ease;
            white-space: nowrap;
            background-color: rgba(255, 255, 255, 0.2);
        }
        .top-nav a:hover {
            background-color: rgba(255, 255, 255, 0.4);
            transform: translateY(-1px);
        }
        /* Reflection and Write-up boxes */
        .reflection-box {
            background-color: #f0f8ff; /* Light blue */
            border-left: 5px solid #add8e6; /* Slightly darker blue vertical line */
            padding: 15px;
            margin: 20px 0;
            border-radius: 8px; /* Consistent border-radius */
        }
        .reflection-box h4 {
            color: #0056b3; /* Consistent h4 color */
            margin-top: 0;
            border-bottom: none;
            padding-bottom: 0;
        }
        .writeup-box {
            background-color: #e9f5e9; /* Light green */
            border-left: 5px solid #4CAF50; /* Slightly darker green vertical line */
            padding: 15px;
            margin: 20px 0;
            border-radius: 8px; /* Consistent border-radius */
        }
        .writeup-box h4 {
            color: #0056b3; /* Consistent h4 color */
            margin-top: 0;
            border-bottom: none;
            padding-bottom: 0;
        }
        hr {
            border: 0;
            height: 2px;
            background: #e0e0e0;
            margin: 40px 0;
        }
    </style>
</head>
<body>
    <div class="top-nav">
        <span>Links to:</span>
        <a href="../index.html">Home</a>
        <a href="../r.html">R Studio Guides</a>
    </div>

    <div class="container">
        <h1>Introductory Statistics Guide: Reliability Analysis & Cronbach's Alpha in R</h1>
        <p>Welcome to this guide on reliability analysis, a crucial aspect of psychological and social science research. When we measure something, like 'satisfaction' or 'anxiety', we often use multiple questions or 'items' to capture the full concept. Reliability analysis helps us understand if these items consistently measure the same thing.</p>

        <h3>What is Reliability?</h3>
        <p>Imagine using a bathroom scale. If you step on it multiple times in a row and get wildly different readings each time, you'd consider it unreliable. In research, reliability refers to the consistency of a measure. If a questionnaire is reliable, it should produce similar results under consistent conditions.</p>

        <h3>Introducing Cronbach's Alpha</h3>
        <p>One of the most common ways to assess the internal consistency reliability of a scale is through <strong>Cronbach's Alpha (&alpha;)</strong>. It's a single number that ranges from 0 to 1, indicating how closely related a set of items are as a group. Think of it as the average correlation of all possible split-halves of a test.</p>
        <ul>
            <li>A higher Cronbach's Alpha generally indicates greater internal consistency.</li>
            <li><strong>Interpretation Guidelines (general):</strong>
                <ul>
                    <li>&alpha; &ge; 0.9: Excellent</li>
                    <li>0.8 &le; &alpha; &lt; 0.9: Good</li>
                    <li>0.7 &le; &alpha; &lt; 0.8: Acceptable</li>
                    <li>0.6 &le; &alpha; &lt; 0.7: Questionable</li>
                    <li>0.5 &le; &alpha; &lt; 0.6: Poor</li>
                    <li>&alpha; &lt; 0.5: Unacceptable</li>
                </ul>
            </li>
        </ul>
        <p>It's important to note that while a high alpha is desirable, it doesn't necessarily mean the scale is measuring what it's supposed to measure (that's validity). It just means the items are consistent with each other.</p>

        <hr>

        <h3>Data Setup: The Omni Dataset</h3>
        <p>This guide uses the omni dataset. If you need help loading this data, please refer to the loading data guides.</p>

        <hr>

        <h3>When Responses Might Need to be Reverse Scored</h3>
        <p>Sometimes, not all questions in a scale are phrased in the same direction. For example, in a satisfaction survey, most questions might ask about positive aspects of satisfaction ("I am happy with...", "I am pleased with..."), but one question might be phrased negatively ("I am dissatisfied with..."). If a high score on the 'satisfied' questions indicates high satisfaction, then a high score on the 'dissatisfied' question would indicate <em>low</em> satisfaction.</p>
        <p>To ensure all items contribute consistently to the overall score, we need to <strong>reverse score</strong> the negatively phrased items. This means transforming the response so that a high score on that item now also indicates high satisfaction.</p>

        <h4>How to identify items for reverse scoring:</h4>
        <ol>
            <li><strong>Examine the wording:</strong> Carefully read each question. If a question is negatively phrased or indicates the opposite construct to the others, it likely needs reverse scoring.</li>
            <li><strong>Check correlations:</strong> If an item is negatively correlated with the other items in the scale, it's a strong indicator that it needs to be reverse scored.</li>
        </ol>
        <p>In your dataset, the <code>satisfaction_q</code> variables all show positive correlations with each other (e.g., satisfaction_q1 and satisfaction_q2 correlate at ~0.88; satisfaction_q3 and satisfaction_q4 at ~0.91). This suggests that all satisfaction questions are phrased in a consistent direction and <strong>do not require reverse scoring</strong> for this specific dataset.</p>

        <h4>Example of Reverse Scoring (if needed in R):</h4>
        <p>If your scale uses a 1-5 Likert scale (1=Strongly Disagree, 5=Strongly Agree), and an item needs reverse scoring (where 1 becomes 5, 2 becomes 4, etc. on a 1-5 scale):</p>
        <p>In your R script, assuming your data is in an object called <code>omni_data</code>, you would use code similar to this:</p>
        <pre><code># Example: Reverse score satisfaction_q3 if it were negatively phrased
# This creates a new variable 'satisfaction_q3_rev'
omni_data$satisfaction_q3_rev &lt;- 6 - omni_data$satisfaction_q3

# You can check the new variable (and its correlation with others)
# cor(omni_data$satisfaction_q3, omni_data$satisfaction_q3_rev) # Should be -1
</code></pre>
        <p><strong>Explanation of the R Code:</strong></p>
        <ul>
            <li><code>omni_data$satisfaction_q3_rev</code>: This creates a new variable within your <code>omni_data</code> dataset, named <code>satisfaction_q3_rev</code>.</li>
            <li><code>&lt;- 6 - omni_data$satisfaction_q3</code>: This is the formula for reverse scoring a 1-5 scale. For each value in <code>satisfaction_q3</code>, R subtracts it from 6. So, 1 becomes 5, 2 becomes 4, etc.</li>
        </ul>
        <p>If you have multiple items to reverse score, you would repeat this line for each item.</p>

        <hr>

        <h3>R How-To: Reliability Analysis (Cronbach's Alpha)</h3>
        <p>To calculate Cronbach's Alpha in R, you'll typically use a package called <code>psych</code>. If you haven't installed it, you'll need to do so first.</p>

        <h4>Step 1: Install and Load the `psych` Package (if not already done)</h4>
        <p>You only need to install a package once per R installation. You need to load it every time you start a new R session where you want to use its functions.</p>
        <pre><code># Install the psych package (only run this line ONCE if you don't have it)
# install.packages("psych")

# Load the psych package (run this every time you start a new R session and want to use it)
library(psych)
</code></pre>

        <h4>Step 2: Calculate Cronbach's Alpha</h4>
        <p>Once the package is loaded, you can use the <code>alpha()</code> function.</p>
        <pre><code># Select only the satisfaction scale items
satisfaction_items &lt;- omni_data[, c("satisfaction_q1", "satisfaction_q2", "satisfaction_q3", "satisfaction_q4", "satisfaction_q5")]

# Calculate Cronbach's Alpha
alpha_results &lt;- alpha(satisfaction_items, check.keys = FALSE) # check.keys=FALSE because no reverse scoring needed
print(alpha_results$total$raw_alpha) # Print just the overall Cronbach's Alpha
# Or print the full results object for more details
# print(alpha_results)
</code></pre>
        <p><strong>Explanation of the R Code:</strong></p>
        <ul>
            <li><code>satisfaction_items &lt;- omni_data[, c(...)]</code>: We create a new R object (a data frame) called <code>satisfaction_items</code> that contains only the variables (columns) we want for our scale.
                <ul>
                    <li><code>omni_data[,]</code>: Selects rows and columns from <code>omni_data</code>.</li>
                    <li>`c("...", "...")`: Creates a "vector" of column names.</li>
                </ul>
            </li>
            <li><code>alpha(satisfaction_items, check.keys = FALSE)</code>: This is the function call.
                <ul>
                    <li><code>satisfaction_items</code>: The data frame containing your scale items.</li>
                    <li><code>check.keys = FALSE</code>: This argument tells the function *not* to automatically look for and reverse-score items. We set this to FALSE because we've determined our items don't need reverse scoring, or we've already done it manually. If you had negatively phrased items you *hadn't* manually reverse-scored, you would set this to `TRUE`.</li>
                </ul>
            </li>
            <li><code>alpha_results$total$raw_alpha</code>: The `alpha()` function returns a complex object. This extracts the main Cronbach's Alpha value.</li>
        </ul>
        <p>For your <code>satisfaction_q</code> items, the Cronbach's Alpha is approximately <strong>0.960</strong>, indicating excellent internal consistency. The full `alpha_results` object (if you print it) provides detailed output, including item statistics (like 'alpha if item dropped'), which are useful for further evaluation.</p>
        <div class="reflection-box">
            <h4>Reflection Question:</h4>
            <p>If your Cronbach's Alpha was very low (e.g., 0.2), what might that suggest about the set of items you used to measure satisfaction?</p>
        </div>

        <hr>

        <h3>R How-To: Create a Combined Scale Score</h3>
        <p>Once you've established the reliability of your scale, you often want to create a single 'combined' or 'total' score for each participant. This score represents their overall standing on the construct being measured (e.g., overall satisfaction).</p>
        <p>You have two main options for creating this combined score: <strong>summing</strong> the item scores or <strong>averaging</strong> them.</p>
        <ul>
            <li>If you <strong>sum</strong> the five <code>satisfaction_q</code> scores (each ranging from 1 to 5), your combined score would range from 5 (1+1+1+1+1) to 25 (5+5+5+5+5).</li>
            <li>If you <strong>average</strong> the five <code>satisfaction_q</code> scores, your combined score will remain within the same range as the original scale, which is 1 to 5. Averaging is generally recommended as it keeps the score on the original scale metric.</li>
        </ul>
        <p>Here's how to create a combined satisfaction score from your <code>satisfaction_q</code> variables in R using the averaging method:</p>
        <pre><code># Create the combined satisfaction score (average of the 5 items)
# The rowMeans function automatically handles NA values by default (na.rm = FALSE)
# To handle NAs by removing them from the mean calculation for each row, use na.rm = TRUE
omni_data$combined_satisfaction &lt;- rowMeans(omni_data[, c("satisfaction_q1", "satisfaction_q2", "satisfaction_q3", "satisfaction_q4", "satisfaction_q5")], na.rm = TRUE)

# View the new variable for the first few rows
head(omni_data$combined_satisfaction)
</code></pre>
        <p><strong>Explanation of the R Code:</strong></p>
        <ul>
            <li><code>omni_data$combined_satisfaction</code>: This creates a new variable in your <code>omni_data</code> dataset called <code>combined_satisfaction</code>.</li>
            <li><code>rowMeans(...)</code>: This function calculates the mean across the specified columns for each row (each participant).</li>
            <li><code>omni_data[, c(...)]</code>: This selects the relevant satisfaction columns from your <code>omni_data</code>.</li>
            <li><code>na.rm = TRUE</code>: This is important! It tells R to *remove* any missing values (NA) for a given row before calculating the mean for that row. If a participant has, for example, 4 out of 5 valid responses, `rowMeans` with `na.rm = TRUE` will calculate the mean of those 4 responses. If you set `na.rm = FALSE` (or omit it, as it's the default for `rowMeans`), any row with even one `NA` will result in an `NA` for the combined score.</li>
        </ul>
        <div class="reflection-box">
            <h4>Reflection Question:</h4>
            <p>What would be an advantage of using the mean (average) for your combined scale score compared to using the sum?</p>
        </div>

        <hr>

        <h3>R How-To: Generate Summary Statistics for the Combined Satisfaction Score</h3>
        <p>Once you have your combined scale, it's good practice for beginner students to generate some summary statistics to understand its distribution. This includes measures like the mean, median, standard deviation, and range.</p>
        <p>Here's how to get summary statistics for your <code>combined_satisfaction</code> score in R:</p>
        <pre><code># Get a quick summary of the combined satisfaction score
summary(omni_data$combined_satisfaction)

# For more specific statistics:
mean(omni_data$combined_satisfaction, na.rm = TRUE)
sd(omni_data$combined_satisfaction, na.rm = TRUE)
min(omni_data$combined_satisfaction, na.rm = TRUE)
max(omni_data$combined_satisfaction, na.rm = TRUE)
median(omni_data$combined_satisfaction, na.rm = TRUE)
</code></pre>
        <p><strong>Explanation of the R Code:</strong></p>
        <ul>
            <li><code>summary(omni_data$combined_satisfaction)</code>: The generic <code>summary()</code> function provides a five-number summary (min, 1st quartile, median, mean, 3rd quartile, max) for numeric vectors.</li>
            <li><code>mean()</code>, <code>sd()</code>, <code>min()</code>, <code>max()</code>, <code>median()</code>: These are specific functions for calculating individual descriptive statistics.</li>
            <li><code>na.rm = TRUE</code>: For all these functions, this argument ensures that missing values (NA) are removed from the calculation. This is crucial if some participants had missing data on some items when the combined score was created.</li>
        </ul>
        <p>For your <code>combined_satisfaction</code> score, the summary statistics are:</p>
        <ul>
            <li><strong>Count:</strong> 100 (number of non-missing observations for this variable)</li>
            <li><strong>Mean:</strong> 3.04</li>
            <li><strong>Standard Deviation (Std. Dev.):</strong> 1.40</li>
            <li><strong>Minimum:</strong> 1.00</li>
            <li><strong>25th Percentile:</strong> 1.60</li>
            <li><strong>Median (50th Percentile):</strong> 3.10</li>
            <li><strong>75th Percentile:</strong> 4.45</li>
            <li><strong>Maximum:</strong> 5.00</li>
        </ul>
        <p>These statistics give you a good overview of your participants' overall satisfaction levels.</p>

        <hr>

        <h3>How to Write-Up Reliability Results</h3>
        <p>When you're writing up your research, it's crucial to clearly report the reliability of your measures. Here's a template for how a beginner student can write up the results for Cronbach's Alpha:</p>
        <div class="writeup-box">
            <h4>Example Write-Up:</h4>
            <p>"The internal consistency reliability of the five-item satisfaction scale was assessed using Cronbach's Alpha. The analysis revealed excellent internal consistency, with a Cronbach's Alpha of &alpha; = 0.96. This indicates that the five items consistently measure the same underlying construct of satisfaction. Given the high internal consistency, the five items were averaged to create a single 'combined satisfaction' score for subsequent analyses."</p>
        </div>
        <h4>Key elements to include:</h4>
        <ul>
            <li><strong>What was assessed:</strong> Clearly state the scale or measure whose reliability was assessed.</li>
            <li><strong>Method:</strong> Specify that Cronbach's Alpha was used.</li>
            <li><strong>Result:</strong> Report the exact Cronbach's Alpha value.</li>
            <li><strong>Interpretation:</strong> Briefly interpret what the alpha value means in the context of your scale (e.g., excellent, good, acceptable).</li>
            <li><strong>Action taken:</strong> If a combined score was created, mention how it was done (e.g., averaged, summed).</li>
        </ul>

    </div>
</body>
</html>
